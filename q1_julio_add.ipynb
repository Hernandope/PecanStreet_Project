{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes quite a bit of time for date-inference\n",
    "# Optimization: Consider manual caching in a dict (top StOvflw answer)\n",
    "data = pd.read_csv(\"PecanStreet_Project/dataport-export_gas_oct2015-mar2016.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many houses are included in the measurement study? Are there any malfunctioning meters? If so, identify them and the time periods where they were malfunctioning. The information below regarding data collection may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique houses: \", len(data[\"dataid\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating Time Difference in Seconds \n",
    "# Caclulating Difference between readings (in order)\n",
    "# Assumption: localminute column is clean and error free\n",
    "merged_df = pd.DataFrame()\n",
    "for k,df in data.groupby([\"dataid\"]): \n",
    "    df.sort_values(by=[\"localminute\"], inplace=True)\n",
    "    df[\"val_diff\"] = df[\"meter_value\"].diff()\n",
    "    df[\"time_diff\"] = pd.to_datetime(df['localminute'], utc=True, infer_datetime_format=True, cache=True).diff().astype('timedelta64[s]')\n",
    "    merged_df = merged_df.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate hourly readings from the raw data. Select one month from the 6-month study interval and plot the hourly readings (time-series) for that month. Hint: You will have to decide what to do if there are no readings for a certain hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How many readings do I have per house?  \n",
    "keys, counts = zip(*[(k, len(df)) for k, df in merged_df.groupby(\"dataid\")])\n",
    "px.scatter(x=keys, y=counts, log_y=True, color=counts, title=\"Number of Readings per House\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like some houses don't have a lot of readings. \n",
    "# Let's extract the house_ids\n",
    "MIN_NUM_READING_THRESHOLD = 50\n",
    "less_readings = [(x, y) for x, y in list(zip(keys, counts)) if y < MIN_NUM_READING_THRESHOLD]\n",
    "less_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: get rid of these houses in less_readings before next cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, means, stds, maxs, mins = zip(*[(k, df[\"val_diff\"].mean(), df[\"val_diff\"].std(), df[\"val_diff\"].max(), df[\"val_diff\"].min()) for k, df in merged_df.groupby(\"dataid\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=keys, y=means, log_y=True, title=\"Mean Meter Diff per House\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=keys, y=maxs, log_y=True, title=\"Max Meter Diff per House\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=keys, y=mins, title=\"Min Meter Diff per House\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interesting. Some houses haev negative diff. Malfunctioning meters? \n",
    "# How many bad readings and how many houses?\n",
    "negative_diff_df = merged_df[merged_df['val_diff'] < 0]\n",
    "print(len(negative_diff_df))                # how many readings\n",
    "print(negative_diff_df['dataid'].nunique()) # how many houses\n",
    "print(negative_diff_df['dataid'].unique())  # which houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When do the spikes happen for each house? \n",
    "# Note that this diff < 0 signifies represents the RIGHT END of a spike, _NOT_ PEAK.\n",
    "spike_dict = {} # dataid: number of spikes, first_spike_time, last_spike_time, diff_first_last_in_hours\n",
    "for k, df in negative_diff_df.groupby('dataid'): \n",
    "    num_spikes = len(df)\n",
    "    first_spike_end = df['localminute'].iloc[0]\n",
    "    last_spike_end = df['localminute'].iloc[-1]\n",
    "    spike_dict[k] = (num_spikes, first_spike_end, last_spike_end, (pd.to_datetime(last_spike_end, utc=True)-pd.to_datetime(first_spike_end, utc=True)))\n",
    "spike_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like it happens throughout the time period we have\n",
    "# TODO: need to create a list for each house for when do the spikes happen? \n",
    "px.scatter(negative_diff_df, x='localminute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, means, stds, maxs, mins = zip(*[(k, df[\"time_diff\"].mean(), df[\"time_diff\"].std(), df[\"time_diff\"].max(), df[\"time_diff\"].min()) for k, df in merged_df.groupby(\"dataid\")])\n",
    "\n",
    "px.scatter(x=keys, y=means, log_y=True, title=\"Mean Time between readings (s) per House\").show()\n",
    "px.scatter(x=keys, y=mins, title=\"Minimum Time between readings (s) per House\").show()\n",
    "px.scatter(x=keys, y=maxs, log_y=True, title=\"Max Time between readings (s) Per House\").show()\n",
    "px.scatter(x=keys, y=stds, log_y=True, title=\"Standard Deviation of Time between (s) readings by House\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resampling per hour\n",
    "\n",
    "resampled_df = pd.DataFrame()\n",
    "\n",
    "for k, df in merged_df.groupby(\"dataid\"): \n",
    "    df = df.set_index(pd.DatetimeIndex(pd.to_datetime(df['localminute'], utc=True, infer_datetime_format=True, cache=True)))\n",
    "    df.drop(columns=[\"localminute\"], inplace=True)\n",
    "\n",
    "    df.drop(columns=[\"val_diff\", \"time_diff\"], inplace=True)\n",
    "\n",
    "    # keep last sample of every hour only\n",
    "    # missing hours become NA\n",
    "    sample = df.resample('1h').last()\n",
    "\n",
    "    # fix data_id for missing rows\n",
    "    sample[\"dataid\"].fillna(k, inplace=True)\n",
    "\n",
    "    # TODO configurable hyperparameter for \"fixing data\"\n",
    "    sample[\"meter_value\"] = sample[\"meter_value\"].interpolate()\n",
    "\n",
    "    resampled_df = resampled_df.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_december_data = resampled_df.loc['2015-12-01':'2015-12-31']\n",
    "px.line(resampled_december_data, x=resampled_december_data.index, y=\"meter_value\", color=\"dataid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Question: How do we clkean the negative readings? \n",
    "#     Option 1: Modify merged_df so that diff = 0 \n",
    "#     Option 2: Get rid of the rows in merged_df so our fill can path it automatically (OPTION CHOSEN IN CODE BELOW)\n",
    "\n",
    "spikeless_resampled_df = pd.DataFrame()\n",
    "\n",
    "for k, df in merged_df.groupby(\"dataid\"): \n",
    "    df = df.set_index(pd.DatetimeIndex(pd.to_datetime(df['localminute'], utc=True, infer_datetime_format=True, cache=True)))\n",
    "    df.drop(columns=[\"localminute\"], inplace=True)\n",
    "\n",
    "    spikeless_df = df[~(df['val_diff'].shift(-1) < 0)] \n",
    "    spikeless_df['val_diff'] = spikeless_df['meter_value'].diff()\n",
    "\n",
    "\n",
    "    # Need to do this because some spikes are less \"sharp\" than 1 timestemp\n",
    "    for i in range(10):  # by right should be doing UNTIL no more spikes left. Tested to see no more spikes after 10 passes\n",
    "        spikeless_df = spikeless_df[~(spikeless_df['val_diff'].shift(-1) < 0)] \n",
    "        spikeless_df['val_diff'] = spikeless_df['meter_value'].diff()\n",
    "\n",
    "    spikeless_df.drop(columns=[\"val_diff\", \"time_diff\"], inplace=True)\n",
    "\n",
    "    spikeless_sample = spikeless_df.resample('1h').last()\n",
    "\n",
    "    spikeless_sample[\"dataid\"].fillna(k, inplace=True)\n",
    "\n",
    "    spikeless_sample[\"meter_value\"] = spikeless_sample[\"meter_value\"].interpolate()\n",
    "\n",
    "    spikeless_resampled_df = spikeless_resampled_df.append(spikeless_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see how spikeless looks now. \n",
    "spikeless_resampled_december_data = spikeless_resampled_df.loc['2015-12-01':'2015-12-31']\n",
    "px.line(spikeless_resampled_december_data, x=spikeless_resampled_december_data.index, y=\"meter_value\", color=\"dataid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.line(spikeless_resampled_df, x=spikeless_resampled_df.index, y=\"meter_value\", color=\"dataid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correlation Running WITH spikes\n",
    "grouped_df = resampled_df.groupby(\"dataid\")\n",
    "top5_pair = {}\n",
    "results = {}\n",
    "for k1, df_1 in grouped_df: \n",
    "    correlations = []\n",
    "    for k2, df_2 in grouped_df: \n",
    "        if k1==k2: \n",
    "            continue \n",
    "        # Does this align data? If not, gotta manually align first. \n",
    "        # What about misaligned data? (eg: meter A started from Oct 10th, while meter B started from Oct 1st. \n",
    "        # Same concern about the last reading)\n",
    "        correlation = df_1[\"meter_value\"].corr(df_2[\"meter_value\"])  \n",
    "        correlations.append((correlation, k2))\n",
    "    correlations.sort(reverse=True)\n",
    "    results[k1] = correlations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# for item in results.items(): \n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation run WITHOUT spikes\n",
    "\n",
    "grouped_df = spikeless_resampled_df.groupby(\"dataid\")\n",
    "\n",
    "results = {}\n",
    "for k1, df_1 in grouped_df: \n",
    "    correlations = []\n",
    "    for k2, df_2 in grouped_df: \n",
    "        if k1==k2: \n",
    "            continue \n",
    "        # Does this align data? If not, gotta manually align first. \n",
    "        # What about misaligned data? (eg: meter A started from Oct 10th, while meter B started from Oct 1st. \n",
    "        # Same concern about the last reading)\n",
    "        correlation = df_1[\"meter_value\"].corr(df_2[\"meter_value\"])  \n",
    "        correlations.append((correlation, k2))\n",
    "    correlations.sort(reverse=True)\n",
    "    results[k1] = correlations[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we expect that gas consumption from different homes to be correlated. For example, many homes would experience higher consumption levels in the evening when meals are cooked. For each home, find the top five homes with which it shows the highest correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3 Corr matrix without spikes, network graph, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results.items(): \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation run WITHOUT spikes and with align\n",
    "\n",
    "grouped_df = spikeless_resampled_df.groupby(\"dataid\")\n",
    "\n",
    "results = {}\n",
    "for k1, df_1 in grouped_df: \n",
    "    correlations = []\n",
    "    for k2, df_2 in grouped_df: \n",
    "        if k1==k2: \n",
    "            continue \n",
    "        # Does this align data? If not, gotta manually align first. \n",
    "        # What about misaligned data? (eg: meter A started from Oct 10th, while meter B started from Oct 1st. \n",
    "        # Same concern about the last reading)\n",
    "        df_1, df_2 = df_1.align(df_2, join='inner', axis=0) #Inner join to align data before calculating corr\n",
    "        correlation = df_1[\"meter_value\"].corr(df_2[\"meter_value\"], method='pearson')  \n",
    "        correlations.append((correlation, k2))\n",
    "    correlations.sort(reverse=True)\n",
    "    results[k1] = correlations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default, no method specified\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dict into df corr matrix\n",
    "corr_df = pd.DataFrame()\n",
    "for key, val in results.items():\n",
    "    for res in val:\n",
    "        corr_df.at[key, res[1]] = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(30, 30))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# cmap=\"YlGnBu\" #alternative color map\n",
    "sns.heatmap(corr_df, cmap=cmap, vmax=.3, center=0, xticklabels=True,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .7})\n",
    "\n",
    "###### plot using matlab (even uglier though)\n",
    "# f = plt.figure(figsize=(19, 15))\n",
    "# plt.matshow(corr_df, fignum=f.number)\n",
    "# plt.xticks(range(corr_df.shape[1]), corr_df.columns, fontsize=14, rotation=45)\n",
    "# plt.yticks(range(corr_df.shape[1]), corr_df.columns, fontsize=14)\n",
    "# cb = plt.colorbar()\n",
    "# cb.ax.tick_params(labelsize=14)\n",
    "# plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use builtin heatmap plotter from pandas\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot network graph source:https://python-graph-gallery.com/327-network-from-correlation-matrix/\n",
    "import networkx as nx\n",
    "# Transform it in a links data frame (3 columns only):\n",
    "links = corr_df.stack().reset_index()\n",
    "links.columns = ['var1', 'var2','value']\n",
    "links\n",
    " \n",
    "# Keep only correlation over a threshold and remove self correlation (cor(A,A)=1)\n",
    "links_filtered=links.loc[ (links['value'] > 0.8) & (links['var1'] != links['var2']) ]\n",
    "links_filtered\n",
    "\n",
    "# Build your graph\n",
    "G=nx.from_pandas_edgelist(links_filtered, 'var1', 'var2')\n",
    "# Plot the network:\n",
    "nx.draw(G, with_labels=True,\n",
    "        node_color='cyan',\n",
    "        node_size=100,\n",
    "        edge_color='black',\n",
    "        linewidths=0.05, font_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#with align\n",
    "for item in results.items(): \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
